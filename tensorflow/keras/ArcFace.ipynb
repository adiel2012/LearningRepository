{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.4 64-bit ('environment': venv)",
   "display_name": "Python 3.8.4 64-bit ('environment': venv)",
   "metadata": {
    "interpreter": {
     "hash": "c7078795451516990b5063858187451c7430e7cd38ad966d2507248d462c4472"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace: Additive Angular Margin Loss for Deep Face Recognition.  \n",
    "[click here](https://arxiv.org/abs/1801.07698) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, X, labels, batch_size) :\n",
    "    self.X = X\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.ceil(len(self.X) / float(self.batch_size))).astype(np.int)\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    batch_x = [self.X[k] for k in indexes]\n",
    "    batch_y = [self.labels[k] for k in indexes] \n",
    "    X = np.array(batch_x, dtype=np.int32)\n",
    "    Y = np.array(batch_y)\n",
    "    #print(Y.shape)\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "     \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "class My_Custom_GeneratorURLs(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size, height, width, num_channels = 3) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.floor(len(self.image_filenames) / float(self.batch_size))).astype(np.int)  # do not  use ceil  because the histogram don know\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    #global height_reshaped, width_reshaped\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    b_size = len(indexes)\n",
    "\n",
    "    batch_x = [self.image_filenames[k] for k in indexes] \n",
    "    batch_y = [self.labels[k] for k in indexes]\n",
    "    #  255.0 * resize(imread(str(file_name)), (height_reshaped, width_reshaped, 3))\n",
    "    X = np.array([ 255.0 * resize(imread(str(file_name)), (self.height, self.width, 3))     #\n",
    "               for file_name in batch_x], dtype=np.int32)\n",
    "    \n",
    "    Y = np.array(batch_y)\n",
    "    X = np.reshape(X, (b_size, self.num_channels * self.height * self.width))\n",
    "\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "    return X, Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape)\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        #x, y = inputs\n",
    "        c = K.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4000, 2)\n(4000, 1)\n(4000, 2)\n[[0]\n [0]\n [0]\n ...\n [1]\n [1]\n [1]]\n"
     ]
    }
   ],
   "source": [
    "num_patterns = 2000\n",
    "num_features = 2\n",
    "mu, sigma = 0, 0.5\n",
    "num_classes = 2\n",
    "num_hidden_neurons = 4\n",
    "Xred = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([1,1])\n",
    "Yred = np.zeros(num_patterns, dtype=int)\n",
    "Xblue = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([-1,-1])\n",
    "Yblue = np.ones(num_patterns, dtype=int)\n",
    "X = np.concatenate((Xred, Xblue), axis=0)\n",
    "Y = np.concatenate((Yred, Yblue), axis=0)\n",
    "Y = np.reshape(Y, (-1,1))\n",
    "\n",
    "Y_hot = np.squeeze(np.eye(num_classes)[Y.reshape(-1)])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y_hot.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 2)\n(None, 2)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "real_input = inputs[:,0:num_features]\n",
    "print(real_input.shape)\n",
    "real_output = inputs[:,num_features: num_features+num_classes]\n",
    "print(real_output.shape)\n",
    "output_1 = layers.Dense(4, activation='relu')(real_input)\n",
    "output_2 = layers.Dense(4, activation='relu')(output_1)\n",
    "predictions = ArcFace(num_classes)(output_2, real_output)  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 8.1743 - accuracy: 0.2131 - val_loss: 7.2452 - val_accuracy: 0.2750\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 6.6231 - accuracy: 0.3417 - val_loss: 6.4885 - val_accuracy: 0.4025\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.9374 - accuracy: 0.4658 - val_loss: 5.9177 - val_accuracy: 0.4900\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.5104 - accuracy: 0.6081 - val_loss: 5.7565 - val_accuracy: 0.6075\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4480 - accuracy: 0.6269 - val_loss: 5.7480 - val_accuracy: 0.6075\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4426 - accuracy: 0.6269 - val_loss: 5.7455 - val_accuracy: 0.6075\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4404 - accuracy: 0.6269 - val_loss: 5.7443 - val_accuracy: 0.6075\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4386 - accuracy: 0.6269 - val_loss: 5.7349 - val_accuracy: 0.6075\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4356 - accuracy: 0.6269 - val_loss: 5.7343 - val_accuracy: 0.6075\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4350 - accuracy: 0.6269 - val_loss: 5.7340 - val_accuracy: 0.6075\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4346 - accuracy: 0.6269 - val_loss: 5.7337 - val_accuracy: 0.6075\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4343 - accuracy: 0.6269 - val_loss: 5.7335 - val_accuracy: 0.6075\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4341 - accuracy: 0.6269 - val_loss: 5.7333 - val_accuracy: 0.6075\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4339 - accuracy: 0.6269 - val_loss: 5.7332 - val_accuracy: 0.6075\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4338 - accuracy: 0.6269 - val_loss: 5.7330 - val_accuracy: 0.6075\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4336 - accuracy: 0.6269 - val_loss: 5.7329 - val_accuracy: 0.6075\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4335 - accuracy: 0.6269 - val_loss: 5.7329 - val_accuracy: 0.6075\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4334 - accuracy: 0.6269 - val_loss: 5.7328 - val_accuracy: 0.6075\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4334 - accuracy: 0.6269 - val_loss: 5.7327 - val_accuracy: 0.6075\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4333 - accuracy: 0.6269 - val_loss: 5.7327 - val_accuracy: 0.6075\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4332 - accuracy: 0.6269 - val_loss: 5.7326 - val_accuracy: 0.6075\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.4332 - accuracy: 0.6269 - val_loss: 5.7326 - val_accuracy: 0.6075\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4331 - accuracy: 0.6269 - val_loss: 5.7325 - val_accuracy: 0.6075\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4331 - accuracy: 0.6269 - val_loss: 5.7325 - val_accuracy: 0.6075\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4331 - accuracy: 0.6269 - val_loss: 5.7325 - val_accuracy: 0.6075\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4330 - accuracy: 0.6269 - val_loss: 5.7324 - val_accuracy: 0.6075\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4330 - accuracy: 0.6269 - val_loss: 5.7324 - val_accuracy: 0.6075\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4330 - accuracy: 0.6269 - val_loss: 5.7324 - val_accuracy: 0.6075\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4329 - accuracy: 0.6269 - val_loss: 5.7324 - val_accuracy: 0.6075\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.4329 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4329 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4329 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7323 - val_accuracy: 0.6075\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4328 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7322 - val_accuracy: 0.6075\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7321 - val_accuracy: 0.6075\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7321 - val_accuracy: 0.6075\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.4327 - accuracy: 0.6269 - val_loss: 5.7321 - val_accuracy: 0.6075\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x254a755d490>"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_hot, test_size=0.1, random_state=34)\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, y_train, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(X_test, y_test, batch_size)\n",
    "\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_texts = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes_texts)\n",
    "height = 32\n",
    "width = 32\n",
    "num_channels = 3\n",
    "batch_size = 8\n",
    "X_train = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(i+1)+'.png')) for name in classes_texts for i in range(7)))\n",
    "X_test  = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(7+i+1)+'.png')) for name in classes_texts for i in range(3))) \n",
    "y_train = list((j for j in range(10) for i in range(7)))\n",
    "y_test = list((j for j in range(10) for i in range(3)))\n",
    "\n",
    "y_train_hot = np.squeeze(np.eye(num_classes)[np.array(y_train).reshape(-1)])\n",
    "y_test_hot = np.squeeze(np.eye(num_classes)[np.array(y_test).reshape(-1)])\n",
    "\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train_hot, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test_hot, batch_size, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = height * width*  num_channels\n",
    "inputs = tf.keras.Input(shape=(num_features + num_classes))\n",
    "\n",
    "#flatten_images = flatten_images.astype('float32')\n",
    "flatten_images = (inputs[:, 0:num_features] / 255.0) \n",
    "# you can reshape output_1 and convert it in the original dataset\n",
    "#flatten_images = tf.reshape(flatten_images, [-1, height, width, num_channels])\n",
    "output_1 = layers.Dense(14, activation='relu')(flatten_images)\n",
    "output_2 = layers.Dense(14, activation='relu')(output_1)\n",
    "\n",
    "# you can compare how faster is the ArcFace decreasing the loss\n",
    "#predictions = layers.Dense(num_classes, activation='softmax')(output_2)\n",
    "predictions = ArcFace(num_classes)(output_2, inputs[:, num_features: num_features + num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========] - 1s 78ms/step - loss: 0.4127 - accuracy: 0.9688 - val_loss: 28.9296 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4208 - accuracy: 0.9688 - val_loss: 30.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 366/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.4403 - accuracy: 0.9688 - val_loss: 29.3420 - val_accuracy: 0.0000e+00\n",
      "Epoch 367/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4103 - accuracy: 0.9688 - val_loss: 29.8031 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.4083 - accuracy: 0.9688 - val_loss: 29.0647 - val_accuracy: 0.0000e+00\n",
      "Epoch 369/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4049 - accuracy: 0.9688 - val_loss: 28.2245 - val_accuracy: 0.0000e+00\n",
      "Epoch 370/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4394 - accuracy: 0.9688 - val_loss: 30.1118 - val_accuracy: 0.0000e+00\n",
      "Epoch 371/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4160 - accuracy: 0.9688 - val_loss: 30.1913 - val_accuracy: 0.0000e+00\n",
      "Epoch 372/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.2674 - accuracy: 0.9844 - val_loss: 28.7365 - val_accuracy: 0.0000e+00\n",
      "Epoch 373/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4283 - accuracy: 0.9688 - val_loss: 29.8879 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.3024 - accuracy: 0.9688 - val_loss: 29.9389 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.9332 - accuracy: 0.8594 - val_loss: 29.2823 - val_accuracy: 0.0417\n",
      "Epoch 376/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.3722 - accuracy: 0.9375 - val_loss: 27.2446 - val_accuracy: 0.0417\n",
      "Epoch 377/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.5702 - accuracy: 0.9219 - val_loss: 30.8401 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/500\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.5402 - accuracy: 0.9375 - val_loss: 29.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/500\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.5429 - accuracy: 0.9219 - val_loss: 30.6628 - val_accuracy: 0.0000e+00\n",
      "Epoch 380/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.5714 - accuracy: 0.9062 - val_loss: 28.7646 - val_accuracy: 0.0000e+00\n",
      "Epoch 381/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 1.1123 - accuracy: 0.8750 - val_loss: 34.4698 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.6538 - accuracy: 0.8594 - val_loss: 27.9202 - val_accuracy: 0.0000e+00\n",
      "Epoch 383/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.4836 - accuracy: 0.9531 - val_loss: 28.8835 - val_accuracy: 0.0000e+00\n",
      "Epoch 384/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.4581 - accuracy: 0.9688 - val_loss: 27.4044 - val_accuracy: 0.0417\n",
      "Epoch 385/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.2639 - accuracy: 0.9844 - val_loss: 30.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 386/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4254 - accuracy: 0.9688 - val_loss: 28.8158 - val_accuracy: 0.0417\n",
      "Epoch 387/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.2478 - accuracy: 0.9844 - val_loss: 28.5682 - val_accuracy: 0.0417\n",
      "Epoch 388/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.2387 - accuracy: 0.9844 - val_loss: 28.3494 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/500\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.4060 - accuracy: 0.9688 - val_loss: 27.0089 - val_accuracy: 0.0417\n",
      "Epoch 390/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.4335 - accuracy: 0.9688 - val_loss: 28.2157 - val_accuracy: 0.0417\n",
      "Epoch 391/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4690 - accuracy: 0.9688 - val_loss: 28.2179 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.4300 - accuracy: 0.9688 - val_loss: 28.1278 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.4051 - accuracy: 0.9688 - val_loss: 29.8505 - val_accuracy: 0.0000e+00\n",
      "Epoch 394/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.2571 - accuracy: 0.9844 - val_loss: 29.3842 - val_accuracy: 0.0000e+00\n",
      "Epoch 395/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.4215 - accuracy: 0.9688 - val_loss: 30.5998 - val_accuracy: 0.0000e+00\n",
      "Epoch 396/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.4539 - accuracy: 0.9531 - val_loss: 31.1762 - val_accuracy: 0.0000e+00\n",
      "Epoch 397/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4657 - accuracy: 0.9531 - val_loss: 29.2745 - val_accuracy: 0.0417\n",
      "Epoch 398/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4523 - accuracy: 0.9688 - val_loss: 28.1698 - val_accuracy: 0.0000e+00\n",
      "Epoch 399/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4158 - accuracy: 0.9688 - val_loss: 28.9235 - val_accuracy: 0.0000e+00\n",
      "Epoch 400/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.2223 - accuracy: 0.9844 - val_loss: 31.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 401/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.4115 - accuracy: 0.9688 - val_loss: 27.2733 - val_accuracy: 0.0000e+00\n",
      "Epoch 402/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.4149 - accuracy: 0.9688 - val_loss: 28.5504 - val_accuracy: 0.0000e+00\n",
      "Epoch 403/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4288 - accuracy: 0.9688 - val_loss: 31.0600 - val_accuracy: 0.0000e+00\n",
      "Epoch 404/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.2357 - accuracy: 0.9844 - val_loss: 29.7174 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4158 - accuracy: 0.9688 - val_loss: 28.3669 - val_accuracy: 0.0000e+00\n",
      "Epoch 406/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4010 - accuracy: 0.9688 - val_loss: 29.5573 - val_accuracy: 0.0000e+00\n",
      "Epoch 407/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.4325 - accuracy: 0.9688 - val_loss: 29.5663 - val_accuracy: 0.0000e+00\n",
      "Epoch 408/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4193 - accuracy: 0.9688 - val_loss: 28.2477 - val_accuracy: 0.0000e+00\n",
      "Epoch 409/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3853 - accuracy: 0.9688 - val_loss: 29.3732 - val_accuracy: 0.0000e+00\n",
      "Epoch 410/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4782 - accuracy: 0.9688 - val_loss: 27.8195 - val_accuracy: 0.0000e+00\n",
      "Epoch 411/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.5187 - accuracy: 0.9375 - val_loss: 30.0760 - val_accuracy: 0.0000e+00\n",
      "Epoch 412/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.5497 - accuracy: 0.9375 - val_loss: 26.9658 - val_accuracy: 0.0000e+00\n",
      "Epoch 413/500\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.2969 - accuracy: 0.9688 - val_loss: 30.9588 - val_accuracy: 0.0417\n",
      "Epoch 414/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.4697 - accuracy: 0.9531 - val_loss: 28.9168 - val_accuracy: 0.0000e+00\n",
      "Epoch 415/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3964 - accuracy: 0.9219 - val_loss: 34.2596 - val_accuracy: 0.0000e+00\n",
      "Epoch 416/500\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.2678 - accuracy: 0.9688 - val_loss: 29.5233 - val_accuracy: 0.0000e+00\n",
      "Epoch 417/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.5629 - accuracy: 0.9531 - val_loss: 30.9252 - val_accuracy: 0.0000e+00\n",
      "Epoch 418/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4600 - accuracy: 0.9688 - val_loss: 27.7994 - val_accuracy: 0.0000e+00\n",
      "Epoch 419/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.5008 - accuracy: 0.9375 - val_loss: 28.0187 - val_accuracy: 0.0000e+00\n",
      "Epoch 420/500\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.5898 - accuracy: 0.8906 - val_loss: 30.4664 - val_accuracy: 0.0000e+00\n",
      "Epoch 421/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4860 - accuracy: 0.9531 - val_loss: 30.8347 - val_accuracy: 0.0000e+00\n",
      "Epoch 422/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.4916 - accuracy: 0.9688 - val_loss: 28.9933 - val_accuracy: 0.0417\n",
      "Epoch 423/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.4231 - accuracy: 0.9531 - val_loss: 29.2789 - val_accuracy: 0.0000e+00\n",
      "Epoch 424/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.4493 - accuracy: 0.9531 - val_loss: 30.6410 - val_accuracy: 0.0417\n",
      "Epoch 425/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4354 - accuracy: 0.9688 - val_loss: 31.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 426/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.4098 - accuracy: 0.9688 - val_loss: 29.7633 - val_accuracy: 0.0000e+00\n",
      "Epoch 427/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.3896 - accuracy: 0.9688 - val_loss: 30.5494 - val_accuracy: 0.0000e+00\n",
      "Epoch 428/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.3913 - accuracy: 0.9688 - val_loss: 29.6579 - val_accuracy: 0.0000e+00\n",
      "Epoch 429/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.3836 - accuracy: 0.9688 - val_loss: 27.9758 - val_accuracy: 0.0000e+00\n",
      "Epoch 430/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3597 - accuracy: 0.9688 - val_loss: 28.9751 - val_accuracy: 0.0000e+00\n",
      "Epoch 431/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3651 - accuracy: 0.9688 - val_loss: 29.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 432/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3991 - accuracy: 0.9688 - val_loss: 29.6055 - val_accuracy: 0.0417\n",
      "Epoch 433/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3646 - accuracy: 0.9688 - val_loss: 31.1670 - val_accuracy: 0.0000e+00\n",
      "Epoch 434/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.2343 - accuracy: 0.9844 - val_loss: 28.0477 - val_accuracy: 0.0000e+00\n",
      "Epoch 435/500\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.3665 - accuracy: 0.9688 - val_loss: 29.0930 - val_accuracy: 0.0000e+00\n",
      "Epoch 436/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3577 - accuracy: 0.9688 - val_loss: 30.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 437/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3644 - accuracy: 0.9688 - val_loss: 27.4420 - val_accuracy: 0.0000e+00\n",
      "Epoch 438/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4085 - accuracy: 0.9688 - val_loss: 30.5356 - val_accuracy: 0.0000e+00\n",
      "Epoch 439/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.4310 - accuracy: 0.9531 - val_loss: 30.1061 - val_accuracy: 0.0000e+00\n",
      "Epoch 440/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.3927 - accuracy: 0.9688 - val_loss: 29.5321 - val_accuracy: 0.0000e+00\n",
      "Epoch 441/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.2452 - accuracy: 0.9844 - val_loss: 29.8860 - val_accuracy: 0.0000e+00\n",
      "Epoch 442/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.4219 - accuracy: 0.9531 - val_loss: 31.5902 - val_accuracy: 0.0000e+00\n",
      "Epoch 443/500\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4736 - accuracy: 0.9688 - val_loss: 29.3894 - val_accuracy: 0.0000e+00\n",
      "Epoch 444/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.5883 - accuracy: 0.9219 - val_loss: 27.5561 - val_accuracy: 0.0000e+00\n",
      "Epoch 445/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.3116 - accuracy: 0.9375 - val_loss: 28.6253 - val_accuracy: 0.0000e+00\n",
      "Epoch 446/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.5220 - accuracy: 0.9062 - val_loss: 28.3863 - val_accuracy: 0.0000e+00\n",
      "Epoch 447/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.4221 - accuracy: 0.9219 - val_loss: 27.5511 - val_accuracy: 0.0000e+00\n",
      "Epoch 448/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4470 - accuracy: 0.9531 - val_loss: 29.8616 - val_accuracy: 0.0000e+00\n",
      "Epoch 449/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.4652 - accuracy: 0.9375 - val_loss: 28.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 450/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.4175 - accuracy: 0.9688 - val_loss: 29.3205 - val_accuracy: 0.0000e+00\n",
      "Epoch 451/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.3797 - accuracy: 0.9688 - val_loss: 29.7637 - val_accuracy: 0.0000e+00\n",
      "Epoch 452/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.3674 - accuracy: 0.9688 - val_loss: 29.8909 - val_accuracy: 0.0417\n",
      "Epoch 453/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.2509 - accuracy: 0.9844 - val_loss: 29.6721 - val_accuracy: 0.0000e+00\n",
      "Epoch 454/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.3799 - accuracy: 0.9688 - val_loss: 29.5053 - val_accuracy: 0.0000e+00\n",
      "Epoch 455/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.3983 - accuracy: 0.9531 - val_loss: 30.1694 - val_accuracy: 0.0000e+00\n",
      "Epoch 456/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.3900 - accuracy: 0.9375 - val_loss: 30.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 457/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.2420 - accuracy: 0.9688 - val_loss: 29.1023 - val_accuracy: 0.0000e+00\n",
      "Epoch 458/500\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.4433 - accuracy: 0.9219 - val_loss: 30.6849 - val_accuracy: 0.0000e+00\n",
      "Epoch 459/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.5461 - accuracy: 0.9375 - val_loss: 30.0681 - val_accuracy: 0.0417\n",
      "Epoch 460/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.4650 - accuracy: 0.9688 - val_loss: 29.8051 - val_accuracy: 0.0417\n",
      "Epoch 461/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.3951 - accuracy: 0.9688 - val_loss: 31.0431 - val_accuracy: 0.0000e+00\n",
      "Epoch 462/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3587 - accuracy: 0.9688 - val_loss: 29.2507 - val_accuracy: 0.0000e+00\n",
      "Epoch 463/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.3739 - accuracy: 0.9688 - val_loss: 29.6322 - val_accuracy: 0.0000e+00\n",
      "Epoch 464/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.2453 - accuracy: 0.9844 - val_loss: 29.1884 - val_accuracy: 0.0000e+00\n",
      "Epoch 465/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.3398 - accuracy: 0.9688 - val_loss: 27.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 466/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.1914 - accuracy: 0.9844 - val_loss: 28.1587 - val_accuracy: 0.0000e+00\n",
      "Epoch 467/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.3369 - accuracy: 0.9688 - val_loss: 28.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 468/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.3192 - accuracy: 0.9688 - val_loss: 31.1056 - val_accuracy: 0.0000e+00\n",
      "Epoch 469/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.2086 - accuracy: 0.9844 - val_loss: 27.3930 - val_accuracy: 0.0000e+00\n",
      "Epoch 470/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 28.0983 - val_accuracy: 0.0417\n",
      "Epoch 471/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4012 - accuracy: 0.9688 - val_loss: 29.3257 - val_accuracy: 0.0000e+00\n",
      "Epoch 472/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.2703 - accuracy: 0.9531 - val_loss: 28.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 473/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.4919 - accuracy: 0.9531 - val_loss: 29.7955 - val_accuracy: 0.0000e+00\n",
      "Epoch 474/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.4224 - accuracy: 0.9219 - val_loss: 28.8907 - val_accuracy: 0.0000e+00\n",
      "Epoch 475/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.7088 - accuracy: 0.8125 - val_loss: 29.7663 - val_accuracy: 0.0000e+00\n",
      "Epoch 476/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.9296 - accuracy: 0.7969 - val_loss: 29.8079 - val_accuracy: 0.0000e+00\n",
      "Epoch 477/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.5063 - accuracy: 0.7500 - val_loss: 32.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 478/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.8332 - accuracy: 0.6406 - val_loss: 32.2968 - val_accuracy: 0.0417\n",
      "Epoch 479/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.3265 - accuracy: 0.7031 - val_loss: 27.4195 - val_accuracy: 0.0000e+00\n",
      "Epoch 480/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.3930 - accuracy: 0.7031 - val_loss: 27.3700 - val_accuracy: 0.0417\n",
      "Epoch 481/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.5077 - accuracy: 0.6719 - val_loss: 30.6318 - val_accuracy: 0.0000e+00\n",
      "Epoch 482/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 2.6866 - accuracy: 0.5625 - val_loss: 30.1110 - val_accuracy: 0.0417\n",
      "Epoch 483/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 2.1020 - accuracy: 0.6094 - val_loss: 30.8532 - val_accuracy: 0.0000e+00\n",
      "Epoch 484/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.0070 - accuracy: 0.8125 - val_loss: 31.2962 - val_accuracy: 0.0000e+00\n",
      "Epoch 485/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.6590 - accuracy: 0.8906 - val_loss: 27.0765 - val_accuracy: 0.0417\n",
      "Epoch 486/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.5631 - accuracy: 0.9062 - val_loss: 28.4495 - val_accuracy: 0.0417\n",
      "Epoch 487/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.7478 - accuracy: 0.8906 - val_loss: 29.3498 - val_accuracy: 0.0417\n",
      "Epoch 488/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4464 - accuracy: 0.9531 - val_loss: 27.8017 - val_accuracy: 0.0417\n",
      "Epoch 489/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.4420 - accuracy: 0.9531 - val_loss: 28.1244 - val_accuracy: 0.0000e+00\n",
      "Epoch 490/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.6060 - accuracy: 0.9375 - val_loss: 28.3940 - val_accuracy: 0.0000e+00\n",
      "Epoch 491/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.4926 - accuracy: 0.9375 - val_loss: 32.2659 - val_accuracy: 0.0000e+00\n",
      "Epoch 492/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.3754 - accuracy: 0.9688 - val_loss: 28.3100 - val_accuracy: 0.0000e+00\n",
      "Epoch 493/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4026 - accuracy: 0.9688 - val_loss: 30.1107 - val_accuracy: 0.0417\n",
      "Epoch 494/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3463 - accuracy: 0.9688 - val_loss: 27.9943 - val_accuracy: 0.0000e+00\n",
      "Epoch 495/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.3365 - accuracy: 0.9688 - val_loss: 31.1949 - val_accuracy: 0.0417\n",
      "Epoch 496/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.3443 - accuracy: 0.9688 - val_loss: 27.8911 - val_accuracy: 0.0000e+00\n",
      "Epoch 497/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.3726 - accuracy: 0.9688 - val_loss: 28.5310 - val_accuracy: 0.0000e+00\n",
      "Epoch 498/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.3309 - accuracy: 0.9688 - val_loss: 29.4287 - val_accuracy: 0.0000e+00\n",
      "Epoch 499/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.1975 - accuracy: 0.9844 - val_loss: 29.6641 - val_accuracy: 0.0000e+00\n",
      "Epoch 500/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3462 - accuracy: 0.9688 - val_loss: 29.9926 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2550737f820>"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "height = 32\n",
    "width = 32\n",
    "num_features = height * width*  num_channels\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train_hot, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test_hot, batch_size, height, width)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(num_features + num_classes))\n",
    "flatten_images = inputs[:, 0:num_features]/ 255.0\n",
    "# you can reshape output_1 and convert it in the original dataset\n",
    "real_input = tf.reshape(flatten_images, [-1, height, width, num_channels])\n",
    "\n",
    "resnet_output = tf.keras.applications.ResNet152V2(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=( height, width, num_channels),\n",
    "    pooling=None,\n",
    "    classes=num_classes,\n",
    "    classifier_activation=None,\n",
    ")(real_input)\n",
    "\n",
    "desired_output = inputs[:, num_features: num_features + num_classes]\n",
    "# you can compare how faster is the ArcFace decreasing the loss\n",
    "#predictions = layers.Dense(num_classes, activation='softmax')(desired_output)\n",
    "predictions = ArcFace(num_classes)(resnet_output, desired_output)  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 21.0782 - accuracy: 0.1094WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\n",
      "8/8 [==============================] - 26s 3s/step - loss: 21.0782 - accuracy: 0.1094 - val_loss: 28.3795 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 27.7786 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 23.8484 - accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 21.3025 - accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 12s 2s/step - loss: 19.0253 - accuracy: 0.0469\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 18.5835 - accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 16.8879 - accuracy: 0.0469\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 12s 2s/step - loss: 16.3570 - accuracy: 0.0312\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 16.1527 - accuracy: 0.0312\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 16.9063 - accuracy: 0.0156\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 14.9406 - accuracy: 0.0625\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 15.7117 - accuracy: 0.0156\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 15.9165 - accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 15.2532 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 15.5511 - accuracy: 0.0469\n",
      "Epoch 16/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 17.6749 - accuracy: 0.0625\n",
      "Epoch 17/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 18.1239 - accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 16.4961 - accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 15.1891 - accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 14.8192 - accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 14.7773 - accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 12.8318 - accuracy: 0.0312\n",
      "Epoch 23/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 11.6542 - accuracy: 0.1562\n",
      "Epoch 24/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 12.3198 - accuracy: 0.1094\n",
      "Epoch 25/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 13.3517 - accuracy: 0.1250\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - 18s 2s/step - loss: 14.5022 - accuracy: 0.0625\n",
      "Epoch 27/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 14.9010 - accuracy: 0.1250\n",
      "Epoch 28/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 14.9510 - accuracy: 0.0781\n",
      "Epoch 29/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 15.1268 - accuracy: 0.0156\n",
      "Epoch 30/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 15.0514 - accuracy: 0.0625\n",
      "Epoch 31/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 13.1882 - accuracy: 0.1406\n",
      "Epoch 32/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 11.7342 - accuracy: 0.1719\n",
      "Epoch 33/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 12.3051 - accuracy: 0.2188\n",
      "Epoch 34/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 11.5227 - accuracy: 0.2031\n",
      "Epoch 35/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 9.1620 - accuracy: 0.2344\n",
      "Epoch 36/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 8.3619 - accuracy: 0.3281\n",
      "Epoch 37/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 10.4546 - accuracy: 0.2500\n",
      "Epoch 38/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 9.6907 - accuracy: 0.2031\n",
      "Epoch 39/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 10.8356 - accuracy: 0.2500\n",
      "Epoch 40/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 8.3718 - accuracy: 0.3750\n",
      "Epoch 41/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 9.1782 - accuracy: 0.3281\n",
      "Epoch 42/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 6.8805 - accuracy: 0.4375\n",
      "Epoch 43/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 8.2046 - accuracy: 0.3438\n",
      "Epoch 44/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 6.6338 - accuracy: 0.3750\n",
      "Epoch 45/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 7.6459 - accuracy: 0.3438\n",
      "Epoch 46/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 5.1708 - accuracy: 0.5312\n",
      "Epoch 47/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 6.1926 - accuracy: 0.5000\n",
      "Epoch 48/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.3033 - accuracy: 0.4688\n",
      "Epoch 49/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 8.8446 - accuracy: 0.4688\n",
      "Epoch 50/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 6.0199 - accuracy: 0.4219\n",
      "Epoch 51/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 6.4055 - accuracy: 0.4844\n",
      "Epoch 52/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.8372 - accuracy: 0.5938\n",
      "Epoch 53/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 5.5992 - accuracy: 0.5781\n",
      "Epoch 54/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.0219 - accuracy: 0.5781\n",
      "Epoch 55/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 4.4416 - accuracy: 0.6719\n",
      "Epoch 56/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 3.6600 - accuracy: 0.6562\n",
      "Epoch 57/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 7.1939 - accuracy: 0.4531\n",
      "Epoch 58/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 6.5734 - accuracy: 0.5156\n",
      "Epoch 59/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.2381 - accuracy: 0.4844\n",
      "Epoch 60/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.6937 - accuracy: 0.6250\n",
      "Epoch 61/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 7.5051 - accuracy: 0.4375\n",
      "Epoch 62/500\n",
      "8/8 [==============================] - 11s 1s/step - loss: 8.5619 - accuracy: 0.4688\n",
      "Epoch 63/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.5344 - accuracy: 0.5156\n",
      "Epoch 64/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 3.5690 - accuracy: 0.6562\n",
      "Epoch 65/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 5.1567 - accuracy: 0.6406\n",
      "Epoch 66/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 3.0995 - accuracy: 0.6250\n",
      "Epoch 67/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 4.8839 - accuracy: 0.6562\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 2.7064 - accuracy: 0.7031\n",
      "Epoch 69/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 4.3914 - accuracy: 0.6719\n",
      "Epoch 70/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.4929 - accuracy: 0.7969\n",
      "Epoch 71/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 4.8191 - accuracy: 0.6719\n",
      "Epoch 72/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.5011 - accuracy: 0.7031\n",
      "Epoch 73/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.0834 - accuracy: 0.7969\n",
      "Epoch 74/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.6585 - accuracy: 0.7344\n",
      "Epoch 75/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.2665 - accuracy: 0.8438\n",
      "Epoch 76/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 2.3748 - accuracy: 0.7969\n",
      "Epoch 77/500\n",
      "8/8 [==============================] - 8s 982ms/step - loss: 3.5596 - accuracy: 0.7031\n",
      "Epoch 78/500\n",
      "8/8 [==============================] - 6s 720ms/step - loss: 1.9377 - accuracy: 0.8125\n",
      "Epoch 79/500\n",
      "8/8 [==============================] - 9s 1s/step - loss: 3.0076 - accuracy: 0.7500\n",
      "Epoch 80/500\n",
      "8/8 [==============================] - 12s 2s/step - loss: 3.7328 - accuracy: 0.6562\n",
      "Epoch 81/500\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.0445 - accuracy: 0.7344\n",
      "Epoch 82/500\n",
      "8/8 [==============================] - 8s 1s/step - loss: 2.0824 - accuracy: 0.8125\n",
      "Epoch 83/500\n",
      "8/8 [==============================] - 7s 871ms/step - loss: 2.5184 - accuracy: 0.7344\n",
      "Epoch 84/500\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.8013 - accuracy: 0.7812\n",
      "Epoch 85/500\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5662 - accuracy: 0.7812\n",
      "Epoch 86/500\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.2804 - accuracy: 0.7500\n",
      "Epoch 87/500\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7595 - accuracy: 0.8594\n",
      "Epoch 88/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5019 - accuracy: 0.7812\n",
      "Epoch 89/500\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1142 - accuracy: 0.7344\n",
      "Epoch 90/500\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.4705 - accuracy: 0.8594\n",
      "Epoch 91/500\n",
      "8/8 [==============================] - 7s 827ms/step - loss: 1.5521 - accuracy: 0.8281\n",
      "Epoch 92/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.1391 - accuracy: 0.8438\n",
      "Epoch 93/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.1458 - accuracy: 0.8906\n",
      "Epoch 94/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1137 - accuracy: 0.8906\n",
      "Epoch 95/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1565 - accuracy: 0.8281\n",
      "Epoch 96/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.5965 - accuracy: 0.8438\n",
      "Epoch 97/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2897 - accuracy: 0.8906\n",
      "Epoch 98/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.9392 - accuracy: 0.8594\n",
      "Epoch 99/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3074 - accuracy: 0.9531\n",
      "Epoch 100/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6938 - accuracy: 0.9531\n",
      "Epoch 101/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.9358 - accuracy: 0.7812\n",
      "Epoch 102/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9601 - accuracy: 0.8906\n",
      "Epoch 103/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5841 - accuracy: 0.8906\n",
      "Epoch 104/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.0383 - accuracy: 0.9062\n",
      "Epoch 105/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4440 - accuracy: 0.9375\n",
      "Epoch 106/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.7517 - accuracy: 0.9531\n",
      "Epoch 107/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1223 - accuracy: 0.9531\n",
      "Epoch 108/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5507 - accuracy: 0.9375\n",
      "Epoch 109/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4817 - accuracy: 0.9219\n",
      "Epoch 110/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3945 - accuracy: 0.9375\n",
      "Epoch 111/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4291 - accuracy: 0.9219\n",
      "Epoch 112/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2909 - accuracy: 0.9219\n",
      "Epoch 113/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2278 - accuracy: 0.9688\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3616 - accuracy: 0.9375\n",
      "Epoch 115/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3023 - accuracy: 0.9531\n",
      "Epoch 116/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.9343 - accuracy: 0.8906\n",
      "Epoch 117/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9029 - accuracy: 0.8750\n",
      "Epoch 118/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3751 - accuracy: 0.9375\n",
      "Epoch 119/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1816 - accuracy: 0.8906\n",
      "Epoch 120/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5078 - accuracy: 0.8906\n",
      "Epoch 121/500\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.7147 - accuracy: 0.8594\n",
      "Epoch 122/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.6045 - accuracy: 0.8281\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0466 - accuracy: 0.8125\n",
      "Epoch 124/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4728 - accuracy: 0.8906\n",
      "Epoch 125/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.9052 - accuracy: 0.9062\n",
      "Epoch 126/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.7228 - accuracy: 0.9062\n",
      "Epoch 127/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.8621 - accuracy: 0.9375\n",
      "Epoch 128/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2596 - accuracy: 0.9688\n",
      "Epoch 129/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.7507 - accuracy: 0.9375\n",
      "Epoch 130/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7845 - accuracy: 0.9062\n",
      "Epoch 131/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 132/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.9509 - accuracy: 0.8594\n",
      "Epoch 133/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2317 - accuracy: 0.9375\n",
      "Epoch 134/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6072 - accuracy: 0.9531\n",
      "Epoch 135/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2274 - accuracy: 0.8750\n",
      "Epoch 136/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0649 - accuracy: 0.9688\n",
      "Epoch 137/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.3485 - accuracy: 0.9375\n",
      "Epoch 138/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1417 - accuracy: 0.9531\n",
      "Epoch 139/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2368 - accuracy: 0.9531\n",
      "Epoch 140/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3983 - accuracy: 0.9219\n",
      "Epoch 141/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.3817 - accuracy: 0.9531\n",
      "Epoch 142/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0625 - accuracy: 0.9844\n",
      "Epoch 143/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.6521 - accuracy: 0.8438\n",
      "Epoch 144/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.7333 - accuracy: 0.9062\n",
      "Epoch 145/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.0329 - accuracy: 0.9062\n",
      "Epoch 146/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.3189 - accuracy: 0.8125\n",
      "Epoch 147/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.8101 - accuracy: 0.9219\n",
      "Epoch 148/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5355 - accuracy: 0.9219\n",
      "Epoch 149/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5178 - accuracy: 0.8750\n",
      "Epoch 150/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.8047 - accuracy: 0.9688\n",
      "Epoch 151/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2557 - accuracy: 0.8906\n",
      "Epoch 152/500\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3848 - accuracy: 0.8906\n",
      "Epoch 153/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.7332 - accuracy: 0.9531\n",
      "Epoch 154/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.8857 - accuracy: 0.9062\n",
      "Epoch 155/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6412 - accuracy: 0.9062\n",
      "Epoch 156/500\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.0903 - accuracy: 0.8750\n",
      "Epoch 157/500\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.9774 - accuracy: 0.8438\n",
      "Epoch 158/500\n",
      "3/8 [==========>...................] - ETA: 7s - loss: 0.2184 - accuracy: 0.9583"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-989997eeaa22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mmy_training_batch_generator_cifar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}