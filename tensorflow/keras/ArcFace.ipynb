{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.4 64-bit ('environment': venv)",
   "display_name": "Python 3.8.4 64-bit ('environment': venv)",
   "metadata": {
    "interpreter": {
     "hash": "c7078795451516990b5063858187451c7430e7cd38ad966d2507248d462c4472"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace: Additive Angular Margin Loss for Deep Face Recognition.  \n",
    "[click here](https://arxiv.org/abs/1801.07698)   \n",
    "# Not Ready Yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, X, labels, batch_size) :\n",
    "    self.X = X\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.ceil(len(self.X) / float(self.batch_size))).astype(np.int)\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    batch_x = [self.X[k] for k in indexes]\n",
    "    batch_y = [self.labels[k] for k in indexes] \n",
    "    X = np.array(batch_x, dtype=np.int32)\n",
    "    Y = np.array(batch_y)\n",
    "    #print(Y.shape)\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "     \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "class My_Custom_GeneratorURLs(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size, height, width, num_channels = 3) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.floor(len(self.image_filenames) / float(self.batch_size))).astype(np.int)  # do not  use ceil  because the histogram don know\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    #global height_reshaped, width_reshaped\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    b_size = len(indexes)\n",
    "\n",
    "    batch_x = [self.image_filenames[k] for k in indexes] \n",
    "    batch_y = [self.labels[k] for k in indexes]\n",
    "    #  255.0 * resize(imread(str(file_name)), (height_reshaped, width_reshaped, 3))\n",
    "    X = np.array([ 255.0 * resize(imread(str(file_name)), (self.height, self.width, 3))     #\n",
    "               for file_name in batch_x], dtype=np.int32)\n",
    "    \n",
    "    Y = np.array(batch_y)\n",
    "    X = np.reshape(X, (b_size, self.num_channels * self.height * self.width))\n",
    "\n",
    "    #Y = np.reshape(Y, (-1,1))\n",
    "    \n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "    return X, Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape)\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        #x, y = inputs\n",
    "        c = K.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4000, 2)\n(4000, 1)\n(4000, 2)\n[[0]\n [0]\n [0]\n ...\n [1]\n [1]\n [1]]\n"
     ]
    }
   ],
   "source": [
    "num_patterns = 2000\n",
    "num_features = 2\n",
    "mu, sigma = 0, 0.5\n",
    "num_classes = 2\n",
    "num_hidden_neurons = 4\n",
    "Xred = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([1,1])\n",
    "Yred = np.zeros(num_patterns, dtype=int)\n",
    "Xblue = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([-1,-1])\n",
    "Yblue = np.ones(num_patterns, dtype=int)\n",
    "X = np.concatenate((Xred, Xblue), axis=0)\n",
    "Y = np.concatenate((Yred, Yblue), axis=0)\n",
    "Y = np.reshape(Y, (-1,1))\n",
    "\n",
    "Y_hot = np.squeeze(np.eye(num_classes)[Y.reshape(-1)])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y_hot.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 2)\n(None, 2)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "real_input = inputs[:,0:num_features]\n",
    "print(real_input.shape)\n",
    "real_output = inputs[:,num_features: num_features+num_classes]\n",
    "print(real_output.shape)\n",
    "output_1 = layers.Dense(4, activation='relu')(real_input)\n",
    "output_2 = layers.Dense(4, activation='relu')(output_1)\n",
    "predictions = ArcFace(num_classes)(output_2, real_output)  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 9.4892 - accuracy: 0.2533 - val_loss: 8.4448 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 7.2776 - accuracy: 0.3236 - val_loss: 6.0708 - val_accuracy: 0.4075\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.2371 - accuracy: 0.4881 - val_loss: 5.7843 - val_accuracy: 0.5150\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 6.0654 - accuracy: 0.4975 - val_loss: 5.6306 - val_accuracy: 0.5150\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.9281 - accuracy: 0.4975 - val_loss: 5.4942 - val_accuracy: 0.5150\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8179 - accuracy: 0.4978 - val_loss: 5.3917 - val_accuracy: 0.5175\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.7468 - accuracy: 0.5697 - val_loss: 5.3399 - val_accuracy: 0.6500\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.7122 - accuracy: 0.6217 - val_loss: 5.3139 - val_accuracy: 0.6500\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.6950 - accuracy: 0.6217 - val_loss: 5.3002 - val_accuracy: 0.6500\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.6858 - accuracy: 0.6217 - val_loss: 5.2927 - val_accuracy: 0.6500\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.6806 - accuracy: 0.6217 - val_loss: 5.2878 - val_accuracy: 0.6500\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.6768 - accuracy: 0.6217 - val_loss: 5.2846 - val_accuracy: 0.6500\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.7622 - accuracy: 0.6217 - val_loss: 5.3801 - val_accuracy: 0.6500\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8128 - accuracy: 0.6217 - val_loss: 5.3783 - val_accuracy: 0.6500\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8114 - accuracy: 0.6217 - val_loss: 5.3769 - val_accuracy: 0.6500\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8102 - accuracy: 0.6217 - val_loss: 5.3758 - val_accuracy: 0.6500\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.8093 - accuracy: 0.6217 - val_loss: 5.3749 - val_accuracy: 0.6500\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.8086 - accuracy: 0.6217 - val_loss: 5.3742 - val_accuracy: 0.6500\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8080 - accuracy: 0.6217 - val_loss: 5.3736 - val_accuracy: 0.6500\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8075 - accuracy: 0.6217 - val_loss: 5.3731 - val_accuracy: 0.6500\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8070 - accuracy: 0.6217 - val_loss: 5.3726 - val_accuracy: 0.6500\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8066 - accuracy: 0.6217 - val_loss: 5.3723 - val_accuracy: 0.6500\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8063 - accuracy: 0.6217 - val_loss: 5.3719 - val_accuracy: 0.6500\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8060 - accuracy: 0.6217 - val_loss: 5.3716 - val_accuracy: 0.6500\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8058 - accuracy: 0.6217 - val_loss: 5.3714 - val_accuracy: 0.6500\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8055 - accuracy: 0.6217 - val_loss: 5.3711 - val_accuracy: 0.6500\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8053 - accuracy: 0.6217 - val_loss: 5.3709 - val_accuracy: 0.6500\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8052 - accuracy: 0.6217 - val_loss: 5.3707 - val_accuracy: 0.6500\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8050 - accuracy: 0.6217 - val_loss: 5.3706 - val_accuracy: 0.6500\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8049 - accuracy: 0.6217 - val_loss: 5.3704 - val_accuracy: 0.6500\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8047 - accuracy: 0.6217 - val_loss: 5.3703 - val_accuracy: 0.6500\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8046 - accuracy: 0.6217 - val_loss: 5.3702 - val_accuracy: 0.6500\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8045 - accuracy: 0.6217 - val_loss: 5.3700 - val_accuracy: 0.6500\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8044 - accuracy: 0.6217 - val_loss: 5.3699 - val_accuracy: 0.6500\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8043 - accuracy: 0.6217 - val_loss: 5.3698 - val_accuracy: 0.6500\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8042 - accuracy: 0.6217 - val_loss: 5.3697 - val_accuracy: 0.6500\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.8041 - accuracy: 0.6217 - val_loss: 5.3697 - val_accuracy: 0.6500\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8040 - accuracy: 0.6217 - val_loss: 5.3696 - val_accuracy: 0.6500\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8040 - accuracy: 0.6217 - val_loss: 5.3695 - val_accuracy: 0.6500\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8039 - accuracy: 0.6217 - val_loss: 5.3694 - val_accuracy: 0.6500\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8039 - accuracy: 0.6217 - val_loss: 5.3694 - val_accuracy: 0.6500\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8038 - accuracy: 0.6217 - val_loss: 5.3693 - val_accuracy: 0.6500\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8037 - accuracy: 0.6217 - val_loss: 5.3693 - val_accuracy: 0.6500\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8037 - accuracy: 0.6217 - val_loss: 5.3692 - val_accuracy: 0.6500\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8036 - accuracy: 0.6217 - val_loss: 5.3692 - val_accuracy: 0.6500\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8036 - accuracy: 0.6217 - val_loss: 5.3691 - val_accuracy: 0.6500\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8036 - accuracy: 0.6217 - val_loss: 5.3691 - val_accuracy: 0.6500\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8035 - accuracy: 0.6217 - val_loss: 5.3690 - val_accuracy: 0.6500\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.8035 - accuracy: 0.6217 - val_loss: 5.3690 - val_accuracy: 0.6500\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8034 - accuracy: 0.6217 - val_loss: 5.3690 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x254c2149940>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_hot, test_size=0.1, random_state=34)\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, y_train, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(X_test, y_test, batch_size)\n",
    "\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_texts = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes_texts)\n",
    "height = 32\n",
    "width = 32\n",
    "num_channels = 3\n",
    "batch_size = 8\n",
    "X_train = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(i+1)+'.png')) for name in classes_texts for i in range(7)))\n",
    "X_test  = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(7+i+1)+'.png')) for name in classes_texts for i in range(3))) \n",
    "y_train = list((j for j in range(10) for i in range(7)))\n",
    "y_test = list((j for j in range(10) for i in range(3)))\n",
    "\n",
    "y_train_hot = np.squeeze(np.eye(num_classes)[np.array(y_train).reshape(-1)])\n",
    "y_test_hot = np.squeeze(np.eye(num_classes)[np.array(y_test).reshape(-1)])\n",
    "\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train_hot, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test_hot, batch_size, height, width)\n",
    "\n",
    "#print(y_train)\n",
    "#print(y_train_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = height * width*  num_channels\n",
    "inputs = tf.keras.Input(shape=(num_features + num_classes))\n",
    "\n",
    "#flatten_images = flatten_images.astype('float32')\n",
    "flatten_images = (inputs[:, 0:num_features] / 255.0) \n",
    "# you can reshape output_1 and convert it in the original dataset\n",
    "#flatten_images = tf.reshape(flatten_images, [-1, height, width, num_channels])\n",
    "output_1 = layers.Dense(14, activation='relu')(flatten_images)\n",
    "output_2 = layers.Dense(14, activation='relu')(output_1)\n",
    "\n",
    "# you can compare how faster is the ArcFace decreasing the loss\n",
    "#predictions = layers.Dense(num_classes, activation='softmax')(output_2)\n",
    "predictions = ArcFace(num_classes)(output_2, inputs[:, num_features: num_features + num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17\n",
      "Epoch 362/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 31.3938 - val_accuracy: 0.0417\n",
      "Epoch 363/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 31.9880 - val_accuracy: 0.0000e+00\n",
      "Epoch 364/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 31.5472 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 30.8393 - val_accuracy: 0.0417\n",
      "Epoch 366/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0544 - accuracy: 1.0000 - val_loss: 33.0053 - val_accuracy: 0.0417\n",
      "Epoch 367/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 33.9903 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 31.7065 - val_accuracy: 0.0417\n",
      "Epoch 369/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 30.6862 - val_accuracy: 0.0417\n",
      "Epoch 370/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 32.1045 - val_accuracy: 0.0417\n",
      "Epoch 371/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 31.9681 - val_accuracy: 0.0417\n",
      "Epoch 372/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 31.8703 - val_accuracy: 0.0417\n",
      "Epoch 373/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 32.7381 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 29.8435 - val_accuracy: 0.0417\n",
      "Epoch 375/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 32.6148 - val_accuracy: 0.0417\n",
      "Epoch 376/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 31.8964 - val_accuracy: 0.0417\n",
      "Epoch 377/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 33.5210 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 31.2335 - val_accuracy: 0.0417\n",
      "Epoch 379/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 31.6765 - val_accuracy: 0.0417\n",
      "Epoch 380/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 31.5457 - val_accuracy: 0.0417\n",
      "Epoch 381/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 32.2868 - val_accuracy: 0.0417\n",
      "Epoch 382/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 30.5482 - val_accuracy: 0.0417\n",
      "Epoch 383/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 32.7850 - val_accuracy: 0.0417\n",
      "Epoch 384/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 31.5003 - val_accuracy: 0.0417\n",
      "Epoch 385/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 30.7401 - val_accuracy: 0.0417\n",
      "Epoch 386/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 29.1527 - val_accuracy: 0.0417\n",
      "Epoch 387/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 31.4761 - val_accuracy: 0.0000e+00\n",
      "Epoch 388/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 34.2002 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 31.5570 - val_accuracy: 0.0417\n",
      "Epoch 390/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 32.8760 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 32.5294 - val_accuracy: 0.0417\n",
      "Epoch 392/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 34.1851 - val_accuracy: 0.0417\n",
      "Epoch 393/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 30.4132 - val_accuracy: 0.0417\n",
      "Epoch 394/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 31.8007 - val_accuracy: 0.0417\n",
      "Epoch 395/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 33.8852 - val_accuracy: 0.0417\n",
      "Epoch 396/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 32.0857 - val_accuracy: 0.0417\n",
      "Epoch 397/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 31.2822 - val_accuracy: 0.0417\n",
      "Epoch 398/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 30.3283 - val_accuracy: 0.0417\n",
      "Epoch 399/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 30.4547 - val_accuracy: 0.0417\n",
      "Epoch 400/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 33.9607 - val_accuracy: 0.0000e+00\n",
      "Epoch 401/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 31.9843 - val_accuracy: 0.0417\n",
      "Epoch 402/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 32.7873 - val_accuracy: 0.0417\n",
      "Epoch 403/500\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 30.5780 - val_accuracy: 0.0417\n",
      "Epoch 404/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 33.8221 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 30.3083 - val_accuracy: 0.0417\n",
      "Epoch 406/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 32.8554 - val_accuracy: 0.0000e+00\n",
      "Epoch 407/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 30.7583 - val_accuracy: 0.0417\n",
      "Epoch 408/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 30.1074 - val_accuracy: 0.0417\n",
      "Epoch 409/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 29.9983 - val_accuracy: 0.0417\n",
      "Epoch 410/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 31.3002 - val_accuracy: 0.0417\n",
      "Epoch 411/500\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 30.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 412/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 31.8753 - val_accuracy: 0.0417\n",
      "Epoch 413/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 33.2242 - val_accuracy: 0.0417\n",
      "Epoch 414/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 33.2543 - val_accuracy: 0.0000e+00\n",
      "Epoch 415/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 31.5690 - val_accuracy: 0.0417\n",
      "Epoch 416/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 32.6425 - val_accuracy: 0.0417\n",
      "Epoch 417/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 30.7479 - val_accuracy: 0.0417\n",
      "Epoch 418/500\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 31.5136 - val_accuracy: 0.0417\n",
      "Epoch 419/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 32.3675 - val_accuracy: 0.0417\n",
      "Epoch 420/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 31.5275 - val_accuracy: 0.0417\n",
      "Epoch 421/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 33.6287 - val_accuracy: 0.0417\n",
      "Epoch 422/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 34.2663 - val_accuracy: 0.0000e+00\n",
      "Epoch 423/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 34.1365 - val_accuracy: 0.0417\n",
      "Epoch 424/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 32.5089 - val_accuracy: 0.0000e+00\n",
      "Epoch 425/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 31.4552 - val_accuracy: 0.0417\n",
      "Epoch 426/500\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 30.3561 - val_accuracy: 0.0417\n",
      "Epoch 427/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 32.9264 - val_accuracy: 0.0000e+00\n",
      "Epoch 428/500\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 31.7262 - val_accuracy: 0.0417\n",
      "Epoch 429/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 32.3326 - val_accuracy: 0.0417\n",
      "Epoch 430/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 30.5374 - val_accuracy: 0.0417\n",
      "Epoch 431/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 32.7605 - val_accuracy: 0.0000e+00\n",
      "Epoch 432/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 31.1988 - val_accuracy: 0.0417\n",
      "Epoch 433/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 34.1990 - val_accuracy: 0.0000e+00\n",
      "Epoch 434/500\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 33.4632 - val_accuracy: 0.0417\n",
      "Epoch 435/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 31.8604 - val_accuracy: 0.0417\n",
      "Epoch 436/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 29.1272 - val_accuracy: 0.0417\n",
      "Epoch 437/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 31.7810 - val_accuracy: 0.0417\n",
      "Epoch 438/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 32.9906 - val_accuracy: 0.0000e+00\n",
      "Epoch 439/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 30.3635 - val_accuracy: 0.0417\n",
      "Epoch 440/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 30.3950 - val_accuracy: 0.0417\n",
      "Epoch 441/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 30.0727 - val_accuracy: 0.0417\n",
      "Epoch 442/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 30.7232 - val_accuracy: 0.0417\n",
      "Epoch 443/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 31.8966 - val_accuracy: 0.0417\n",
      "Epoch 444/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 31.1979 - val_accuracy: 0.0417\n",
      "Epoch 445/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 32.4641 - val_accuracy: 0.0417\n",
      "Epoch 446/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 29.2133 - val_accuracy: 0.0417\n",
      "Epoch 447/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 31.2528 - val_accuracy: 0.0417\n",
      "Epoch 448/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 31.5444 - val_accuracy: 0.0417\n",
      "Epoch 449/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 31.5901 - val_accuracy: 0.0417\n",
      "Epoch 450/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 31.5117 - val_accuracy: 0.0417\n",
      "Epoch 451/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 30.5919 - val_accuracy: 0.0417\n",
      "Epoch 452/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 32.9045 - val_accuracy: 0.0000e+00\n",
      "Epoch 453/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 30.8736 - val_accuracy: 0.0417\n",
      "Epoch 454/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 33.2798 - val_accuracy: 0.0417\n",
      "Epoch 455/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 30.7231 - val_accuracy: 0.0417\n",
      "Epoch 456/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 31.5664 - val_accuracy: 0.0417\n",
      "Epoch 457/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 31.5132 - val_accuracy: 0.0417\n",
      "Epoch 458/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 31.9463 - val_accuracy: 0.0000e+00\n",
      "Epoch 459/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 29.7630 - val_accuracy: 0.0417\n",
      "Epoch 460/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 32.4332 - val_accuracy: 0.0417\n",
      "Epoch 461/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 31.2897 - val_accuracy: 0.0417\n",
      "Epoch 462/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 31.7305 - val_accuracy: 0.0417\n",
      "Epoch 463/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 32.3592 - val_accuracy: 0.0417\n",
      "Epoch 464/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 31.6037 - val_accuracy: 0.0417\n",
      "Epoch 465/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 31.3098 - val_accuracy: 0.0417\n",
      "Epoch 466/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 31.5002 - val_accuracy: 0.0417\n",
      "Epoch 467/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 31.9135 - val_accuracy: 0.0417\n",
      "Epoch 468/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 31.7343 - val_accuracy: 0.0417\n",
      "Epoch 469/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 31.2769 - val_accuracy: 0.0417\n",
      "Epoch 470/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 33.3672 - val_accuracy: 0.0000e+00\n",
      "Epoch 471/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 33.4235 - val_accuracy: 0.0000e+00\n",
      "Epoch 472/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 31.4156 - val_accuracy: 0.0417\n",
      "Epoch 473/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 31.4034 - val_accuracy: 0.0000e+00\n",
      "Epoch 474/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 34.1813 - val_accuracy: 0.0000e+00\n",
      "Epoch 475/500\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 30.4851 - val_accuracy: 0.0417\n",
      "Epoch 476/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 31.6171 - val_accuracy: 0.0417\n",
      "Epoch 477/500\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 30.0034 - val_accuracy: 0.0417\n",
      "Epoch 478/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 32.5556 - val_accuracy: 0.0417\n",
      "Epoch 479/500\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 31.2384 - val_accuracy: 0.0417\n",
      "Epoch 480/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 31.1585 - val_accuracy: 0.0417\n",
      "Epoch 481/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 31.4054 - val_accuracy: 0.0000e+00\n",
      "Epoch 482/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 31.3335 - val_accuracy: 0.0000e+00\n",
      "Epoch 483/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 31.6313 - val_accuracy: 0.0417\n",
      "Epoch 484/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 30.8326 - val_accuracy: 0.0417\n",
      "Epoch 485/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 31.6856 - val_accuracy: 0.0417\n",
      "Epoch 486/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 32.7213 - val_accuracy: 0.0000e+00\n",
      "Epoch 487/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 30.1301 - val_accuracy: 0.0417\n",
      "Epoch 488/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 30.0260 - val_accuracy: 0.0417\n",
      "Epoch 489/500\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 30.3784 - val_accuracy: 0.0417\n",
      "Epoch 490/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 29.9685 - val_accuracy: 0.0417\n",
      "Epoch 491/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 30.7323 - val_accuracy: 0.0417\n",
      "Epoch 492/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 31.3628 - val_accuracy: 0.0417\n",
      "Epoch 493/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 31.9496 - val_accuracy: 0.0417\n",
      "Epoch 494/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 29.8013 - val_accuracy: 0.0417\n",
      "Epoch 495/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 31.6379 - val_accuracy: 0.0417\n",
      "Epoch 496/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 31.5856 - val_accuracy: 0.0417\n",
      "Epoch 497/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 31.0320 - val_accuracy: 0.0417\n",
      "Epoch 498/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 31.5669 - val_accuracy: 0.0000e+00\n",
      "Epoch 499/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 31.6510 - val_accuracy: 0.0417\n",
      "Epoch 500/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 32.7994 - val_accuracy: 0.0417\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x254a753fc40>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "height = 32\n",
    "width = 32\n",
    "num_features = height * width*  num_channels\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test, batch_size, height, width)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(num_features + num_classes))\n",
    "flatten_images = inputs[:, 0:num_features]/ 255.0\n",
    "# you can reshape output_1 and convert it in the original dataset\n",
    "flatten_images = tf.reshape(flatten_images, [-1, height, width, num_channels])\n",
    "\n",
    "flatten_images = tf.keras.applications.ResNet152V2(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=( height, width, num_channels),\n",
    "    pooling=None,\n",
    "    classes=num_classes,\n",
    "    classifier_activation=None,\n",
    ")(flatten_images)\n",
    "\n",
    "\n",
    "# you can compare how faster is the ArcFace decreasing the loss\n",
    "#predictions = layers.Dense(num_classes, activation='softmax')(output_2)\n",
    "predictions = ArcFace(num_classes)(flatten_images, inputs[:, num_features: num_features + num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-205ea41cb252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mmy_training_batch_generator_cifar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enqueuer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m     super(KerasSequenceAdapter, self).__init__(\n\u001b[0m\u001b[0;32m    910\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Shuffle is handed in the _make_callable override.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Learning\\LearningExamples\\LearningRepository\\environment\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    918\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
      "\u001b[1;32m<ipython-input-48-77352c9d6a69>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m#Y = np.reshape(Y, (-1,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}