{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598718791668",
   "display_name": "Python 3.8.4 64-bit ('environment': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace: Additive Angular Margin Loss for Deep Face Recognition.  \n",
    "[click here](https://arxiv.org/abs/1801.07698)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, X, labels, batch_size) :\n",
    "    self.X = X\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.ceil(len(self.X) / float(self.batch_size))).astype(np.int)\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    batch_x = [self.X[k] for k in indexes]\n",
    "    batch_y = [self.labels[k] for k in indexes] \n",
    "    X = np.array(batch_x, dtype=np.int32)\n",
    "    Y = np.array(batch_y)\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "     \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "class My_Custom_GeneratorURLs(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size, height, width, num_channels = 3) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.floor(len(self.image_filenames) / float(self.batch_size))).astype(np.int)  # do not  use ceil  because the histogram don know\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    #global height_reshaped, width_reshaped\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    b_size = len(indexes)\n",
    "\n",
    "    batch_x = [self.image_filenames[k] for k in indexes] \n",
    "    batch_y = [self.labels[k] for k in indexes]\n",
    "    #  255.0 * resize(imread(str(file_name)), (height_reshaped, width_reshaped, 3))\n",
    "    X = np.array([ 255.0 * imread(str(file_name))     #\n",
    "               for file_name in batch_x], dtype=np.int32)\n",
    "    \n",
    "    Y = np.array(batch_y)\n",
    "    X = np.reshape(X, (b_size, self.num_channels * self.height * self.width))\n",
    "    Y = np.reshape(Y, (-1,1))\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "    return X, Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace1D(layers.Layer):\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace1D, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace1D, self).build(input_shape)\n",
    "        #print('wwwwwwwww')\n",
    "        #print(input_shape)\n",
    "        #print(input_shape[-1])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        #x, y = inputs\n",
    "        c = K.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape)\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        #x, y = inputs\n",
    "        c = K.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4000, 2)\n(4000, 1)\n[[0]\n [0]\n [0]\n ...\n [1]\n [1]\n [1]]\n"
    }
   ],
   "source": [
    "num_patterns = 2000\n",
    "num_features = 2\n",
    "mu, sigma = 0, 0.5\n",
    "num_classes = 2\n",
    "num_hidden_neurons = 4\n",
    "Xred = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([1,1])\n",
    "Yred = np.zeros(num_patterns, dtype=int)\n",
    "Xblue = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([-1,-1])\n",
    "Yblue = np.ones(num_patterns, dtype=int)\n",
    "X = np.concatenate((Xred, Xblue), axis=0)\n",
    "Y = np.concatenate((Yred, Yblue), axis=0)\n",
    "Y = np.reshape(Y, (-1,1))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "output_1 = layers.Dense(4, activation='relu')(inputs[:,0:num_features])\n",
    "output_2 = layers.Dense(4, activation='relu')(output_1)\n",
    "predictions = ArcFace1D(num_classes)(output_2, inputs[:,num_features: num_features+num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n36/36 [==============================] - 1s 24ms/step - loss: 2.2766 - accuracy: 0.6558 - val_loss: 1.3260 - val_accuracy: 0.6225\nEpoch 2/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.9351 - accuracy: 0.4839 - val_loss: 0.7449 - val_accuracy: 0.3750\nEpoch 3/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.7278 - accuracy: 0.3331 - val_loss: 0.7242 - val_accuracy: 0.4100\nEpoch 4/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.3697 - val_loss: 0.7189 - val_accuracy: 0.3575\nEpoch 5/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.7162 - accuracy: 0.3786 - val_loss: 0.7163 - val_accuracy: 0.2725\nEpoch 6/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.7141 - accuracy: 0.3450 - val_loss: 0.7145 - val_accuracy: 0.2675\nEpoch 7/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.3136 - val_loss: 0.7132 - val_accuracy: 0.2675\nEpoch 8/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.7114 - accuracy: 0.3256 - val_loss: 0.7121 - val_accuracy: 0.1150\nEpoch 9/50\n32/36 [=========================>....] - ETA: 0s - loss: 0.7118 - accuracy: 0.2836/36 [==============================] - 0s 6ms/step - loss: 0.7105 - accuracy: 0.3106 - val_loss: 0.7109 - val_accuracy: 0.3675\nEpoch 10/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7094 - accuracy: 0.4353 - val_loss: 0.7100 - val_accuracy: 0.5200\nEpoch 11/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7085 - accuracy: 0.3767 - val_loss: 0.7090 - val_accuracy: 0.3675\nEpoch 12/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7076 - accuracy: 0.4053 - val_loss: 0.7081 - val_accuracy: 0.3675\nEpoch 13/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7067 - accuracy: 0.4442 - val_loss: 0.7072 - val_accuracy: 0.3675\nEpoch 14/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7058 - accuracy: 0.4258 - val_loss: 0.7063 - val_accuracy: 0.3575\nEpoch 15/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7050 - accuracy: 0.3997 - val_loss: 0.7052 - val_accuracy: 0.3575\nEpoch 16/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7041 - accuracy: 0.3892 - val_loss: 0.7046 - val_accuracy: 0.3575\nEpoch 17/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.3853 - val_loss: 0.7035 - val_accuracy: 0.3575\nEpoch 18/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7025 - accuracy: 0.3803 - val_loss: 0.7029 - val_accuracy: 0.1150\nEpoch 19/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7018 - accuracy: 0.3756 - val_loss: 0.7020 - val_accuracy: 0.2325\nEpoch 20/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7010 - accuracy: 0.3439 - val_loss: 0.7012 - val_accuracy: 0.2775\nEpoch 21/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.7003 - accuracy: 0.3553 - val_loss: 0.7005 - val_accuracy: 0.2825\nEpoch 22/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.6997 - accuracy: 0.3083 - val_loss: 0.6998 - val_accuracy: 0.2725\nEpoch 23/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.6990 - accuracy: 0.3142 - val_loss: 0.6992 - val_accuracy: 0.0300\nEpoch 24/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.6985 - accuracy: 0.2967 - val_loss: 0.6986 - val_accuracy: 0.5250\nEpoch 25/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.6979 - accuracy: 0.3264 - val_loss: 0.6981 - val_accuracy: 0.0300\nEpoch 26/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.2775 - val_loss: 0.6975 - val_accuracy: 0.0300\nEpoch 27/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6970 - accuracy: 0.3025 - val_loss: 0.6971 - val_accuracy: 0.0300\nEpoch 28/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.3208 - val_loss: 0.6967 - val_accuracy: 0.2725\nEpoch 29/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6965 - accuracy: 0.3311 - val_loss: 0.6964 - val_accuracy: 0.5250\nEpoch 30/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6960 - accuracy: 0.3147 - val_loss: 0.6960 - val_accuracy: 0.6600\nEpoch 31/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.3544 - val_loss: 0.6957 - val_accuracy: 0.6600\nEpoch 32/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6954 - accuracy: 0.3594 - val_loss: 0.6956 - val_accuracy: 0.5200\nEpoch 33/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.3503 - val_loss: 0.6953 - val_accuracy: 0.5200\nEpoch 34/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6950 - accuracy: 0.3703 - val_loss: 0.6950 - val_accuracy: 0.2825\nEpoch 35/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6948 - accuracy: 0.3269 - val_loss: 0.6948 - val_accuracy: 0.1650\nEpoch 36/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.3706 - val_loss: 0.6947 - val_accuracy: 0.4175\nEpoch 37/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.3761 - val_loss: 0.6946 - val_accuracy: 0.2675\nEpoch 38/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.3672 - val_loss: 0.6944 - val_accuracy: 0.2675\nEpoch 39/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.3831 - val_loss: 0.6943 - val_accuracy: 0.1650\nEpoch 40/50\n36/36 [==============================] - 0s 5ms/step - loss: 0.6942 - accuracy: 0.3803 - val_loss: 0.6942 - val_accuracy: 0.2675\nEpoch 41/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6942 - accuracy: 0.3169 - val_loss: 0.6941 - val_accuracy: 0.1650\nEpoch 42/50\n36/36 [==============================] - 0s 4ms/step - loss: 0.6944 - accuracy: 0.3653 - val_loss: 0.6941 - val_accuracy: 0.0250\nEpoch 43/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6942 - accuracy: 0.3344 - val_loss: 0.6940 - val_accuracy: 0.2675\nEpoch 44/50\n36/36 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.2964 - val_loss: 0.6939 - val_accuracy: 0.6600\nEpoch 45/50\n36/36 [==============================] - 0s 8ms/step - loss: 0.6938 - accuracy: 0.3764 - val_loss: 0.6938 - val_accuracy: 0.6600\nEpoch 46/50\n36/36 [==============================] - 0s 11ms/step - loss: 0.6938 - accuracy: 0.3342 - val_loss: 0.6938 - val_accuracy: 0.2675\nEpoch 47/50\n36/36 [==============================] - 0s 8ms/step - loss: 0.6937 - accuracy: 0.3494 - val_loss: 0.6937 - val_accuracy: 0.4175\nEpoch 48/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.3972 - val_loss: 0.6937 - val_accuracy: 0.6600\nEpoch 49/50\n36/36 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.4172 - val_loss: 0.6936 - val_accuracy: 0.2725\nEpoch 50/50\n36/36 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.4078 - val_loss: 0.6936 - val_accuracy: 0.7775\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1c85fb08250>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=34)\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, y_train, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(X_test, y_test, batch_size)\n",
    "\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_texts = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes_texts)\n",
    "height = 32\n",
    "width = 32\n",
    "num_channels = 3\n",
    "batch_size = 8\n",
    "X_train = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(i+1)+'.png')) for name in classes_texts for i in range(7)))\n",
    "X_test  = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(7+i+1)+'.png')) for name in classes_texts for i in range(3))) \n",
    "y_train = list((j for j in range(10) for i in range(7)))\n",
    "y_test = list((j for j in range(10) for i in range(3)))\n",
    "\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test, batch_size, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_features = height* width* num_channels\n",
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "output_1 = layers.Dense(44, activation='relu')(inputs[:,0:num_features])\n",
    "output_2 = layers.Dense(44, activation='relu')(output_1)\n",
    "predictions = ArcFace1D(num_classes)(output_2, inputs[:,num_features: num_features+num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n7/8 [=========================>....] - ETA: 0s - loss: 308.5225 - accuracy: 0.0000e+00WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\n8/8 [==============================] - 1s 117ms/step - loss: 327.0363 - accuracy: 0.0000e+00 - val_loss: 396.8424 - val_accuracy: 0.0000e+00\nEpoch 2/50\n8/8 [==============================] - 0s 36ms/step - loss: 405.0330 - accuracy: 0.0000e+00\nEpoch 3/50\n8/8 [==============================] - 0s 23ms/step - loss: 394.1513 - accuracy: 0.0156\nEpoch 4/50\n8/8 [==============================] - 0s 22ms/step - loss: 450.7028 - accuracy: 0.0000e+00\nEpoch 5/50\n8/8 [==============================] - 0s 20ms/step - loss: 459.8130 - accuracy: 0.0000e+00\nEpoch 6/50\n8/8 [==============================] - 0s 22ms/step - loss: 481.4299 - accuracy: 0.0000e+00\nEpoch 7/50\n8/8 [==============================] - 0s 17ms/step - loss: 494.4612 - accuracy: 0.0469\nEpoch 8/50\n8/8 [==============================] - 0s 14ms/step - loss: 492.8427 - accuracy: 0.0938\nEpoch 9/50\n8/8 [==============================] - 0s 16ms/step - loss: 531.9174 - accuracy: 0.1094\nEpoch 10/50\n8/8 [==============================] - 0s 18ms/step - loss: 521.0497 - accuracy: 0.0625\nEpoch 11/50\n8/8 [==============================] - 0s 16ms/step - loss: 548.7719 - accuracy: 0.0938\nEpoch 12/50\n8/8 [==============================] - 0s 14ms/step - loss: 593.3716 - accuracy: 0.1094\nEpoch 13/50\n8/8 [==============================] - 0s 15ms/step - loss: 586.1678 - accuracy: 0.1250\nEpoch 14/50\n8/8 [==============================] - 0s 20ms/step - loss: 572.3499 - accuracy: 0.0781\nEpoch 15/50\n8/8 [==============================] - 0s 16ms/step - loss: 579.5963 - accuracy: 0.1250\nEpoch 16/50\n8/8 [==============================] - 0s 14ms/step - loss: 584.3264 - accuracy: 0.0938\nEpoch 17/50\n8/8 [==============================] - 0s 16ms/step - loss: 630.7426 - accuracy: 0.1406\nEpoch 18/50\n8/8 [==============================] - 0s 13ms/step - loss: 669.4458 - accuracy: 0.1094\nEpoch 19/50\n8/8 [==============================] - 0s 12ms/step - loss: 653.6754 - accuracy: 0.0938\nEpoch 20/50\n8/8 [==============================] - 0s 12ms/step - loss: 683.1322 - accuracy: 0.1094\nEpoch 21/50\n8/8 [==============================] - 0s 14ms/step - loss: 691.8335 - accuracy: 0.1094\nEpoch 22/50\n8/8 [==============================] - 0s 13ms/step - loss: 733.0759 - accuracy: 0.1094\nEpoch 23/50\n8/8 [==============================] - 0s 13ms/step - loss: 745.9379 - accuracy: 0.0938\nEpoch 24/50\n8/8 [==============================] - 0s 12ms/step - loss: 811.9283 - accuracy: 0.1094\nEpoch 25/50\n8/8 [==============================] - 0s 12ms/step - loss: 803.7014 - accuracy: 0.0625\nEpoch 26/50\n8/8 [==============================] - 0s 12ms/step - loss: 854.2095 - accuracy: 0.0469\nEpoch 27/50\n8/8 [==============================] - 0s 12ms/step - loss: 870.6505 - accuracy: 0.0156\nEpoch 28/50\n8/8 [==============================] - 0s 12ms/step - loss: 978.8641 - accuracy: 0.0156\nEpoch 29/50\n8/8 [==============================] - 0s 11ms/step - loss: 940.4717 - accuracy: 0.0156\nEpoch 30/50\n8/8 [==============================] - 0s 12ms/step - loss: 984.6404 - accuracy: 0.0000e+00\nEpoch 31/50\n8/8 [==============================] - 0s 11ms/step - loss: 934.0814 - accuracy: 0.0469\nEpoch 32/50\n8/8 [==============================] - 0s 11ms/step - loss: 954.3051 - accuracy: 0.0625\nEpoch 33/50\n8/8 [==============================] - 0s 12ms/step - loss: 893.3049 - accuracy: 0.1094\nEpoch 34/50\n8/8 [==============================] - 0s 12ms/step - loss: 780.6071 - accuracy: 0.0938\nEpoch 35/50\n8/8 [==============================] - 0s 13ms/step - loss: 755.8876 - accuracy: 0.1094\nEpoch 36/50\n8/8 [==============================] - 0s 11ms/step - loss: 721.9875 - accuracy: 0.0938\nEpoch 37/50\n8/8 [==============================] - 0s 11ms/step - loss: 688.0527 - accuracy: 0.0938\nEpoch 38/50\n8/8 [==============================] - 0s 12ms/step - loss: 634.6401 - accuracy: 0.0938\nEpoch 39/50\n8/8 [==============================] - 0s 12ms/step - loss: 600.7389 - accuracy: 0.0938\nEpoch 40/50\n8/8 [==============================] - 0s 12ms/step - loss: 598.6924 - accuracy: 0.1094\nEpoch 41/50\n8/8 [==============================] - 0s 11ms/step - loss: 554.6602 - accuracy: 0.0781\nEpoch 42/50\n8/8 [==============================] - 0s 11ms/step - loss: 519.9116 - accuracy: 0.0625\nEpoch 43/50\n8/8 [==============================] - 0s 11ms/step - loss: 494.7988 - accuracy: 0.0625\nEpoch 44/50\n8/8 [==============================] - 0s 12ms/step - loss: 451.4252 - accuracy: 0.0625\nEpoch 45/50\n8/8 [==============================] - 0s 24ms/step - loss: 421.8323 - accuracy: 0.0469\nEpoch 46/50\n8/8 [==============================] - 0s 29ms/step - loss: 396.4344 - accuracy: 0.0781\nEpoch 47/50\n8/8 [==============================] - 0s 31ms/step - loss: 388.0288 - accuracy: 0.0469\nEpoch 48/50\n8/8 [==============================] - 0s 29ms/step - loss: 353.4139 - accuracy: 0.0156\nEpoch 49/50\n8/8 [==============================] - 0s 22ms/step - loss: 333.1278 - accuracy: 0.0625\nEpoch 50/50\n8/8 [==============================] - 0s 22ms/step - loss: 321.3433 - accuracy: 0.0312\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1c86257d580>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(100, 12298)\n(100, 12288)\n(100, 10)\n(100, 3, 64, 64)\n"
    }
   ],
   "source": [
    "X1 = np.random.random((100,3,64,64))\n",
    "X1 = np.reshape(X1, (100,3*64*64))\n",
    "Y = np.random.random((100,10))\n",
    "X1 = np.concatenate((X1, Y), axis=1)\n",
    "\n",
    "X2 = X1[:,0: 3*64*64]\n",
    "Y2 = X1[:,3*64*64:3*64*64+10]\n",
    "print(X1.shape)\n",
    "print(X2.shape)\n",
    "print(Y2.shape)\n",
    "\n",
    "X2 = np.reshape(X2,[-1,3,64,64])\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}