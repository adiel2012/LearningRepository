{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598718791668",
   "display_name": "Python 3.8.4 64-bit ('environment': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace: Additive Angular Margin Loss for Deep Face Recognition.  \n",
    "[click here](https://arxiv.org/abs/1801.07698)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, X, labels, batch_size) :\n",
    "    self.X = X\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.ceil(len(self.X) / float(self.batch_size))).astype(np.int)\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    batch_x = [self.X[k] for k in indexes]\n",
    "    batch_y = [self.labels[k] for k in indexes] \n",
    "    X = np.array(batch_x, dtype=np.int32)\n",
    "    Y = np.array(batch_y)\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "     \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "class My_Custom_GeneratorURLs(tf.keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size, height, width, num_channels = 3) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = True\n",
    "    self.on_epoch_end()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "    for item in (self[i] for i in range(len(self))):\n",
    "      yield item  \n",
    "    \n",
    "  def __len__(self) :\n",
    "    #self.on_epoch_end()\n",
    "    result = (np.floor(len(self.image_filenames) / float(self.batch_size))).astype(np.int)  # do not  use ceil  because the histogram don know\n",
    "    return result\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.labels))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    #global height_reshaped, width_reshaped\n",
    "    indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "    b_size = len(indexes)\n",
    "\n",
    "    batch_x = [self.image_filenames[k] for k in indexes] \n",
    "    batch_y = [self.labels[k] for k in indexes]\n",
    "    #  255.0 * resize(imread(str(file_name)), (height_reshaped, width_reshaped, 3))\n",
    "    X = np.array([ 255.0 * imread(str(file_name))     #\n",
    "               for file_name in batch_x], dtype=np.int32)\n",
    "    \n",
    "    Y = np.array(batch_y)\n",
    "    X = np.reshape(X, (b_size, self.num_channels * self.height * self.width))\n",
    "    Y = np.reshape(Y, (-1,1))\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "    return X, Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape)\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        #x, y = inputs\n",
    "        c = K.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        # sin = tf.sqrt(1 - logits**2)\n",
    "        # cos_m = tf.cos(logits)\n",
    "        # sin_m = tf.sin(logits)\n",
    "        # target_logits = logits * cos_m - sin * sin_m\n",
    "        #\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4000, 2)\n(4000, 1)\n[[0]\n [0]\n [0]\n ...\n [1]\n [1]\n [1]]\n"
    }
   ],
   "source": [
    "num_patterns = 2000\n",
    "num_features = 2\n",
    "mu, sigma = 0, 0.5\n",
    "num_classes = 2\n",
    "num_hidden_neurons = 4\n",
    "Xred = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([1,1])\n",
    "Yred = np.zeros(num_patterns, dtype=int)\n",
    "Xblue = np.random.normal(mu, sigma, (num_patterns, num_features)) + np.array([-1,-1])\n",
    "Yblue = np.ones(num_patterns, dtype=int)\n",
    "X = np.concatenate((Xred, Xblue), axis=0)\n",
    "Y = np.concatenate((Yred, Yblue), axis=0)\n",
    "Y = np.reshape(Y, (-1,1))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "output_1 = layers.Dense(4, activation='relu')(inputs[:,0:num_features])\n",
    "output_2 = layers.Dense(4, activation='relu')(output_1)\n",
    "predictions = ArcFace(num_classes)(output_2, inputs[:,num_features: num_features+num_classes])  \n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n36/36 [==============================] - 1s 20ms/step - loss: 3.0265 - accuracy: 0.6308 - val_loss: 2.4874 - val_accuracy: 0.6325\nEpoch 2/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.4092 - accuracy: 0.6606 - val_loss: 2.2916 - val_accuracy: 1.0000\nEpoch 3/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3595 - accuracy: 0.8200 - val_loss: 2.2891 - val_accuracy: 0.5050\nEpoch 4/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3574 - accuracy: 0.5022 - val_loss: 2.2883 - val_accuracy: 0.5050\nEpoch 5/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3562 - accuracy: 0.5022 - val_loss: 2.2878 - val_accuracy: 0.5050\nEpoch 6/50\n36/36 [==============================] - 0s 7ms/step - loss: 2.3554 - accuracy: 0.5022 - val_loss: 2.2875 - val_accuracy: 0.5050\nEpoch 7/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3550 - accuracy: 0.5022 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 8/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.5022 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 9/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.5567 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 10/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.5022 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 11/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.5022 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 12/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.5819 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 13/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.6947 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 14/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.7819 - val_loss: 2.2874 - val_accuracy: 0.5050\nEpoch 15/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.7319 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 16/50\n36/36 [==============================] - 0s 7ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 17/50\n36/36 [==============================] - 0s 9ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 18/50\n36/36 [==============================] - 0s 9ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 19/50\n36/36 [==============================] - 0s 9ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 20/50\n36/36 [==============================] - 0s 7ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 21/50\n36/36 [==============================] - 0s 7ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 22/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 23/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 24/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 25/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 26/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 27/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 28/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 29/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 30/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 31/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 32/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 33/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 34/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 35/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 36/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 37/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 38/50\n36/36 [==============================] - 0s 5ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 39/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 40/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 41/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 42/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 43/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 44/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 45/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 46/50\n36/36 [==============================] - 0s 3ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 47/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 48/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 49/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\nEpoch 50/50\n36/36 [==============================] - 0s 4ms/step - loss: 2.3548 - accuracy: 0.8742 - val_loss: 2.2874 - val_accuracy: 0.8700\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x12235438d30>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=34)\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, y_train, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(X_test, y_test, batch_size)\n",
    "\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_texts = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes_texts)\n",
    "height = 32\n",
    "width = 32\n",
    "num_channels = 3\n",
    "batch_size = 8\n",
    "X_train = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(i+1)+'.png')) for name in classes_texts for i in range(7)))\n",
    "X_test  = list(((os.path.abspath('./../../assets/cifar10small/'+ name+str(7+i+1)+'.png')) for name in classes_texts for i in range(3))) \n",
    "y_train = list((j for j in range(10) for i in range(7)))\n",
    "y_test = list((j for j in range(10) for i in range(3)))\n",
    "\n",
    "my_training_batch_generator_cifar = My_Custom_GeneratorURLs(X_train, y_train, batch_size, height, width)\n",
    "my_validation_batch_generator_cifar = My_Custom_GeneratorURLs(X_test, y_test, batch_size, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = height* width* num_channels\n",
    "inputs = tf.keras.Input(shape=(num_features+num_classes))\n",
    "flatten_images = inputs[:,0:num_features]\n",
    "# you can reshape output_1 and convert it in the original dataset\n",
    "#flatten_images = tf.reshape(flatten_images, [-1, height, width, num_channels])\n",
    "output_1 = layers.Dense(4, activation='relu')(flatten_images)\n",
    "output_2 = layers.Dense(4, activation='relu')(output_1)\n",
    "\n",
    "\n",
    "# you can compare how faster is the ArcFace decreasing the loss\n",
    "#predictions = layers.Dense(num_classes, activation='softmax')(output_2)\n",
    "predictions = ArcFace(num_classes)(output_2, inputs[:,num_features: num_features+num_classes])  \n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n5/8 [=================>............] - ETA: 0s - loss: 1123.1401 - accuracy: 0.1000WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001223B1BEC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\n8/8 [==============================] - 1s 110ms/step - loss: 1176.3855 - accuracy: 0.0625 - val_loss: 1378.0396 - val_accuracy: 0.0000e+00\nEpoch 2/50\n8/8 [==============================] - 0s 17ms/step - loss: 1282.8187 - accuracy: 0.0000e+00\nEpoch 3/50\n8/8 [==============================] - 0s 21ms/step - loss: 1319.5010 - accuracy: 0.0000e+00\nEpoch 4/50\n8/8 [==============================] - 0s 21ms/step - loss: 1406.3425 - accuracy: 0.0000e+00\nEpoch 5/50\n8/8 [==============================] - 0s 16ms/step - loss: 1440.0917 - accuracy: 0.0000e+00\nEpoch 6/50\n8/8 [==============================] - 0s 13ms/step - loss: 1533.8894 - accuracy: 0.0000e+00\nEpoch 7/50\n8/8 [==============================] - 0s 15ms/step - loss: 1553.8169 - accuracy: 0.0000e+00\nEpoch 8/50\n8/8 [==============================] - 0s 16ms/step - loss: 1605.0444 - accuracy: 0.0000e+00\nEpoch 9/50\n8/8 [==============================] - 0s 17ms/step - loss: 1598.3324 - accuracy: 0.0000e+00\nEpoch 10/50\n8/8 [==============================] - 0s 16ms/step - loss: 1590.7827 - accuracy: 0.0000e+00\nEpoch 11/50\n8/8 [==============================] - 0s 14ms/step - loss: 1528.3087 - accuracy: 0.0000e+00\nEpoch 12/50\n8/8 [==============================] - 0s 15ms/step - loss: 1421.4597 - accuracy: 0.0000e+00\nEpoch 13/50\n8/8 [==============================] - 0s 16ms/step - loss: 1312.0643 - accuracy: 0.0000e+00\nEpoch 14/50\n8/8 [==============================] - 0s 16ms/step - loss: 1242.4608 - accuracy: 0.0000e+00\nEpoch 15/50\n8/8 [==============================] - 0s 17ms/step - loss: 1239.6436 - accuracy: 0.0000e+00\nEpoch 16/50\n8/8 [==============================] - 0s 15ms/step - loss: 1291.9989 - accuracy: 0.0000e+00\nEpoch 17/50\n8/8 [==============================] - 0s 19ms/step - loss: 1356.1111 - accuracy: 0.0000e+00\nEpoch 18/50\n8/8 [==============================] - 0s 16ms/step - loss: 1314.1790 - accuracy: 0.0000e+00\nEpoch 19/50\n8/8 [==============================] - 0s 13ms/step - loss: 1324.8075 - accuracy: 0.0000e+00\nEpoch 20/50\n8/8 [==============================] - 0s 14ms/step - loss: 1369.1101 - accuracy: 0.0000e+00\nEpoch 21/50\n8/8 [==============================] - 0s 13ms/step - loss: 1351.9254 - accuracy: 0.0000e+00\nEpoch 22/50\n8/8 [==============================] - 0s 14ms/step - loss: 1364.4207 - accuracy: 0.0000e+00\nEpoch 23/50\n8/8 [==============================] - 0s 11ms/step - loss: 1382.4136 - accuracy: 0.0000e+00\nEpoch 24/50\n8/8 [==============================] - 0s 30ms/step - loss: 1355.7433 - accuracy: 0.0000e+00\nEpoch 25/50\n8/8 [==============================] - 0s 29ms/step - loss: 1359.7031 - accuracy: 0.0000e+00\nEpoch 26/50\n8/8 [==============================] - 0s 25ms/step - loss: 1359.7598 - accuracy: 0.0000e+00\nEpoch 27/50\n8/8 [==============================] - 0s 25ms/step - loss: 1330.2125 - accuracy: 0.0000e+00\nEpoch 28/50\n8/8 [==============================] - 0s 30ms/step - loss: 1285.9215 - accuracy: 0.0000e+00\nEpoch 29/50\n8/8 [==============================] - 0s 22ms/step - loss: 1254.3929 - accuracy: 0.0000e+00\nEpoch 30/50\n8/8 [==============================] - 0s 21ms/step - loss: 1154.4113 - accuracy: 0.0000e+00\nEpoch 31/50\n8/8 [==============================] - 0s 19ms/step - loss: 1147.2957 - accuracy: 0.0000e+00\nEpoch 32/50\n8/8 [==============================] - 0s 21ms/step - loss: 1128.2255 - accuracy: 0.0000e+00\nEpoch 33/50\n8/8 [==============================] - 0s 20ms/step - loss: 1116.0449 - accuracy: 0.0000e+00\nEpoch 34/50\n8/8 [==============================] - 0s 20ms/step - loss: 999.4423 - accuracy: 0.0000e+00\nEpoch 35/50\n8/8 [==============================] - 0s 22ms/step - loss: 1006.2759 - accuracy: 0.0000e+00\nEpoch 36/50\n8/8 [==============================] - 0s 19ms/step - loss: 924.3942 - accuracy: 0.0000e+00\nEpoch 37/50\n8/8 [==============================] - 0s 17ms/step - loss: 890.4383 - accuracy: 0.0000e+00\nEpoch 38/50\n8/8 [==============================] - 0s 13ms/step - loss: 928.1317 - accuracy: 0.0000e+00\nEpoch 39/50\n8/8 [==============================] - 0s 14ms/step - loss: 820.2867 - accuracy: 0.0000e+00\nEpoch 40/50\n8/8 [==============================] - 0s 17ms/step - loss: 838.7935 - accuracy: 0.0000e+00\nEpoch 41/50\n8/8 [==============================] - 0s 19ms/step - loss: 777.0104 - accuracy: 0.0000e+00\nEpoch 42/50\n8/8 [==============================] - 0s 17ms/step - loss: 748.2500 - accuracy: 0.0000e+00\nEpoch 43/50\n8/8 [==============================] - 0s 16ms/step - loss: 692.1729 - accuracy: 0.0000e+00\nEpoch 44/50\n8/8 [==============================] - 0s 22ms/step - loss: 695.5186 - accuracy: 0.0000e+00\nEpoch 45/50\n8/8 [==============================] - 0s 23ms/step - loss: 636.0632 - accuracy: 0.0000e+00\nEpoch 46/50\n8/8 [==============================] - 0s 23ms/step - loss: 640.2483 - accuracy: 0.0000e+00\nEpoch 47/50\n8/8 [==============================] - 0s 22ms/step - loss: 623.3063 - accuracy: 0.0000e+00\nEpoch 48/50\n8/8 [==============================] - 0s 25ms/step - loss: 596.7703 - accuracy: 0.0000e+00\nEpoch 49/50\n8/8 [==============================] - 0s 27ms/step - loss: 597.4434 - accuracy: 0.0000e+00\nEpoch 50/50\n8/8 [==============================] - 0s 23ms/step - loss: 580.5962 - accuracy: 0.0000e+00\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1223ae51ee0>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  my_training_batch_generator_cifar,\n",
    "  epochs=num_epochs,\n",
    "  validation_data = my_validation_batch_generator_cifar,\n",
    "  validation_steps=len(my_validation_batch_generator)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}